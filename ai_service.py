import os
import json
import re
import aiohttp
import asyncio
from typing import Dict, Optional, List
import google.generativeai as genai
from config import settings

# Initialize Gemini with robust error handling (Keep as fallback/alternative)
def initialize_gemini():
    """Initialize Gemini with available models"""
    try:
        if not settings.GEMINI_API_KEY:
            print("âš ï¸ Gemini API key not found")
            return None, False

        genai.configure(api_key=settings.GEMINI_API_KEY)

        # Models to try in order of preference
        working_models = [
            'models/gemini-2.5-flash',
            'models/gemini-2.0-flash',
            'models/gemini-1.5-flash',
            'models/gemini-pro'
        ]

        for model_name in working_models:
            try:
                print(f"ðŸ”„ Testing Gemini model: {model_name}")
                model = genai.GenerativeModel(model_name)
                # Simple test generation
                response = model.generate_content("Test", generation_config={'max_output_tokens': 5})
                if response:
                    print(f"âœ… Successfully initialized: {model_name}")
                    return model, True
            except Exception as e:
                print(f"âš ï¸ Model {model_name} failed: {e}")
                continue

        print("âŒ No Gemini models available")
        return None, False

    except Exception as e:
        print(f"âŒ Gemini initialization failed: {e}")
        return None, False

# Initialize Gemini (Optional now)
gemini_model, GEMINI_ENABLED = initialize_gemini()

class FreeAIService:
    def __init__(self):
        # Enhanced API key validation
        self.groq_api_key = getattr(settings, 'GROQ_API_KEY', None)
        self.deepseek_enabled = bool(settings.DEEPSEEK_API_KEY)
        self.openrouter_enabled = bool(settings.OPENROUTER_API_KEY)
        self.groq_enabled = bool(self.groq_api_key and len(str(self.groq_api_key).strip()) > 0)

        # Debug logging
        print("=" * 60)
        print("ðŸ” API KEY VALIDATION:")
        print(f"   Groq Enabled: {self.groq_enabled}")
        print(f"   DeepSeek Enabled: {self.deepseek_enabled}")
        print(f"   OpenRouter Enabled: {self.openrouter_enabled}")
        print(f"   Gemini Enabled: {GEMINI_ENABLED}")
        print("=" * 60)

    async def generate_topic_recommendations(self) -> List[Dict]:
        """Generate trending interview topics for 2025 using Gemini"""

        # Fallback topics if AI fails
        fallback_topics = [
            {"topic": "AI Agents & LLMs", "trend": "High Demand", "icon": "fa-robot"},
            {"topic": "Rust Programming", "trend": "Growing Fast", "icon": "fa-cogs"},
            {"topic": "Cloud Security", "trend": "Critical", "icon": "fa-shield-alt"},
            {"topic": "React Server Components", "trend": "Standard", "icon": "fa-code"},
            {"topic": "System Design (Scalability)", "trend": "Evergreen", "icon": "fa-sitemap"}
        ]

        if not GEMINI_ENABLED:
            print("âš ï¸ Gemini disabled, using fallback recommendations")
            return fallback_topics

        prompt = """Suggest 5 trending software engineering interview topics for 2025.
Focus on emerging tech like AI, Edge Computing, etc.

Return ONLY a valid JSON array with this structure:
[
    {
        "topic": "Topic Name",
        "trend": "Why it's trending (2-3 words)",
        "icon": "FontAwesome class (e.g., fa-robot)"
    }
]"""

        try:
            print("ðŸ”® Generating recommendations with Gemini...")
            response = await asyncio.to_thread(
                gemini_model.generate_content,
                prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=0.7,
                    max_output_tokens=1000,
                )
            )

            if response.text:
                json_match = re.search(r'\[\s*\{.*\}\s*\]', response.text, re.DOTALL)
                if json_match:
                    return json.loads(json_match.group(0))

            print("âŒ Failed to parse Gemini response for recommendations")
            return fallback_topics

        except Exception as e:
            print(f"âŒ Gemini Recommendation Error: {e}")
            return fallback_topics

    async def generate_quiz(
        self,
        topic: str,
        difficulty: str = "medium",
        question_count: int = 10,
        focus_areas: str = ""
    ) -> Dict:
        """Generate quiz using Gemini (Primary)"""
        if not settings.ENABLE_QUIZ:
            return {"error": "Quiz feature is disabled"}

        question_count = min(question_count, settings.MAX_QUIZ_QUESTIONS)

        if GEMINI_ENABLED:
            print(f"ðŸ”„ Generating Quiz with Gemini for: {topic}")
            prompt = f"""Create {question_count} multiple-choice interview questions about "{topic}" at {difficulty} difficulty level.
{f"Focus areas: {focus_areas}" if focus_areas else ""}

Return ONLY valid JSON array with this exact structure:
[
    {{
        "id": 1,
        "question": "Question text?",
        "options": ["A", "B", "C", "D"],
        "correctAnswer": 0,
        "explanation": "Why?",
        "hint": "Hint",
        "type": "technical",
        "difficulty": "{difficulty}"
    }}
]"""
            try:
                response = await asyncio.to_thread(
                    gemini_model.generate_content,
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.7,
                        max_output_tokens=4000,
                    )
                )

                if response.text:
                    # Clean up markdown code blocks if present
                    clean_text = response.text.replace('```json', '').replace('```', '').strip()
                    json_match = re.search(r'\[\s*\{.*\}\s*\]', clean_text, re.DOTALL)
                    if json_match:
                        questions = json.loads(json_match.group(0))
                        return {
                            "topic": topic,
                            "difficulty": difficulty,
                            "questions": questions,
                            "totalQuestions": len(questions),
                            "source": "gemini"
                        }
            except Exception as e:
                print(f"âŒ Gemini Quiz Gen Error: {e}")

        # Fallback to local
        return self._generate_enhanced_local_quiz(topic, difficulty, question_count, focus_areas)

    async def generate_questions(
        self,
        topic: str,
        job_description: str,
        interview_type: str,
        company_nature: str
    ) -> List[Dict]:
        """Generate interview questions using Gemini (Primary)"""

        prompt = f"""Generate 8 professional interview questions for:
TOPIC: {topic}
JOB ROLE: {job_description}
INTERVIEW TYPE: {interview_type}
COMPANY TYPE: {company_nature}

Create a mix of:
- 3 Technical questions
- 3 Behavioral questions
- 2 Situational questions

Return ONLY a valid JSON array with this exact structure:
[
    {{
        "question": "Question text?",
        "type": "technical",
        "difficulty": "medium",
        "explanation": "Brief explanation of what this evaluates"
    }}
]"""

        if GEMINI_ENABLED:
            print(f"ðŸ”„ Generating Questions with Gemini for: {topic}")
            try:
                response = await asyncio.to_thread(
                    gemini_model.generate_content,
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.7,
                        max_output_tokens=2000,
                    )
                )

                print(f"âœ… Gemini Response Received")
                if response.text:
                    print(f"ðŸ“ Raw Response Length: {len(response.text)} chars")
                    # Clean up markdown code blocks if present
                    clean_text = response.text.replace('```json', '').replace('```', '').strip()
                    json_match = re.search(r'\[\s*\{.*\}\s*\]', clean_text, re.DOTALL)
                    if json_match:
                        questions = json.loads(json_match.group(0))
                        print(f"âœ… Successfully parsed {len(questions)} questions")
                        return questions
                    else:
                        print(f"âŒ No JSON array found in response. First 200 chars: {clean_text[:200]}")
                else:
                    print("âŒ Empty response from Gemini")
            except json.JSONDecodeError as e:
                print(f"âŒ JSON Parse Error: {e}")
                print(f"   Attempted to parse: {clean_text[:500] if 'clean_text' in locals() else 'N/A'}")
            except Exception as e:
                print(f"âŒ Gemini Question Gen Error: {type(e).__name__}: {e}")

        # Fallback to local
        print("âš ï¸ Using local fallback for questions")
        return await generate_local_fallback_questions(topic, job_description, interview_type, company_nature)

    async def generate_analytics(self, user_data: Dict) -> Dict:
        """Generate analytics insights using Gemini"""

        prompt = f"""Analyze this user's interview preparation performance and provide insights:
User Data: {json.dumps(user_data)}

Return ONLY a valid JSON object with this structure:
{{
    "performance_trend": [65, 70, ...], // 7 data points representing progress
    "topic_mastery": {{"Topic A": 80, "Topic B": 60}},
    "average_score": 75,
    "recommendations": ["Rec 1", "Rec 2", "Rec 3"],
    "strength_areas": ["Area 1", "Area 2"],
    "weakness_areas": ["Area 1", "Area 2"]
}}
"""
        if GEMINI_ENABLED:
            print(f"ðŸ”„ Generating Analytics with Gemini")
            try:
                response = await asyncio.to_thread(
                    gemini_model.generate_content,
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.7,
                        max_output_tokens=1000,
                    )
                )

                if response.text:
                    clean_text = response.text.replace('```json', '').replace('```', '').strip()
                    json_match = re.search(r'\{.*\}', clean_text, re.DOTALL)
                    if json_match:
                        return json.loads(json_match.group(0))
            except Exception as e:
                print(f"âŒ Gemini Analytics Error: {e}")

        # Fallback Analytics
        return {
            "performance_trend": [60, 65, 70, 72, 75, 78, 80],
            "topic_mastery": {"General": 70},
            "average_score": 70,
            "recommendations": ["Keep practicing to improve your score.", "Try different topics."],
            "strength_areas": ["Consistency"],
            "weakness_areas": ["Complex topics"]
        }

    async def send_chat_message(self, topic: str, message: str, history: List[Dict] = []) -> str:
        """Send chat message using Groq (Primary)"""

        system_prompt = f"You are AjiEasy AI, an expert interview coach specialized in {topic}. Provide helpful, concise, and encouraging advice."

        messages = [{"role": "system", "content": system_prompt}]

        # Add history (limit to last 5 turns to save tokens)
        for msg in history[-5:]:
            messages.append({"role": msg["role"], "content": msg["content"]})

        messages.append({"role": "user", "content": message})

        if self.groq_enabled:
            try:
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        "https://api.groq.com/openai/v1/chat/completions",
                        headers={
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {self.groq_api_key}"
                        },
                        json={
                            "model": "llama-3.1-70b-versatile",
                            "messages": messages,
                            "temperature": 0.7,
                            "max_tokens": 1000
                        },
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            return data["choices"][0]["message"]["content"]
            except Exception as e:
                print(f"âŒ Groq Chat Error: {e}")

        # Fallback to OpenRouter
        if self.openrouter_enabled:
             return await self._generate_openrouter_chat(messages)

        return "I'm currently offline, but keep practicing! You're doing great."

    async def _call_groq_api(self, prompt: str, max_tokens: int = 1000, json_mode: bool = False) -> Optional[str]:
        """Helper to call Groq API"""
        async with aiohttp.ClientSession() as session:
            payload = {
                "model": "llama-3.1-70b-versatile",
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": max_tokens
            }
            if json_mode:
                payload["response_format"] = {"type": "json_object"}

            async with session.post(
                "https://api.groq.com/openai/v1/chat/completions",
                headers={
                    "Content-Type": "application/json",
                    "Authorization": f"Bearer {self.groq_api_key}"
                },
                json=payload,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    return data["choices"][0]["message"]["content"]
                else:
                    print(f"âŒ Groq API Error: {response.status} - {await response.text()}")
                    return None

    async def _generate_groq_quiz(self, topic: str, difficulty: str, question_count: int, focus_areas: str) -> Dict:
        prompt = f"""Create {question_count} multiple-choice interview questions about "{topic}" at {difficulty} difficulty level.
{f"Focus areas: {focus_areas}" if focus_areas else ""}

Return ONLY valid JSON array with this exact structure:
[
    {{
        "id": 1,
        "question": "Question text?",
        "options": ["A", "B", "C", "D"],
        "correctAnswer": 0,
        "explanation": "Why?",
        "hint": "Hint",
        "type": "technical",
        "difficulty": "{difficulty}"
    }}
]"""
        try:
            response = await self._call_groq_api(prompt, max_tokens=4000)
            if response:
                json_match = re.search(r'\[\s*\{.*\}\s*\]', response, re.DOTALL)
                if json_match:
                    questions = json.loads(json_match.group(0))
                    return {
                        "topic": topic,
                        "difficulty": difficulty,
                        "questions": questions,
                        "totalQuestions": len(questions),
                        "source": "groq"
                    }
        except Exception as e:
            print(f"Groq Quiz Error: {e}")
        return {"error": "Failed to generate quiz"}

    async def _generate_deepseek_quiz(
        self,
        topic: str,
        difficulty: str,
        question_count: int,
        focus_areas: str
    ) -> Dict:
        """Generate quiz using DeepSeek API with enhanced error handling"""
        prompt = f"""Create {question_count} multiple-choice interview questions about {topic} at {difficulty} difficulty.
{f"Focus on: {focus_areas}" if focus_areas else ""}

Return ONLY valid JSON array with this exact structure:
[
    {{
        "id": 1,
        "question": "Clear question text?",
        "options": ["Option A", "Option B", "Option C", "Option D"],
        "correctAnswer": 0,
        "explanation": "Why this is correct",
        "hint": "Helpful hint",
        "type": "technical",
        "difficulty": "{difficulty}"
    }}
]

Make questions practical for job interviews. Include variety."""

        try:
            print(f"ðŸ” Attempting DeepSeek quiz generation for: {topic}")

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.deepseek.com/v1/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {settings.DEEPSEEK_API_KEY}"
                    },
                    json={
                        "model": "deepseek-chat",
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.7,
                        "max_tokens": 4000
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:

                    print(f"ðŸ“¡ DeepSeek Response Status: {response.status}")

                    if response.status == 200:
                        data = await response.json()
                        content = data["choices"][0]["message"]["content"]
                        print(f"ðŸ“ Raw DeepSeek Response: {content[:200]}...")

                        json_match = re.search(r'\[\s*\{.*\}\s*\]', content, re.DOTALL)
                        if json_match:
                            try:
                                questions = json.loads(json_match.group(0))
                                print(f"âœ… DeepSeek generated {len(questions)} questions")
                                return {
                                    "topic": topic,
                                    "difficulty": difficulty,
                                    "questions": questions,
                                    "totalQuestions": len(questions),
                                    "source": "deepseek"
                                }
                            except json.JSONDecodeError as e:
                                print(f"âŒ DeepSeek JSON parse error: {e}")
                                return {"error": "Failed to parse DeepSeek response"}
                        else:
                            print("âŒ No JSON array found in DeepSeek response")
                            return {"error": "No valid JSON in DeepSeek response"}

                    elif response.status == 401:
                        print("âŒ DeepSeek: Invalid API key")
                        return {"error": "DeepSeek API key invalid"}
                    elif response.status == 402:
                        print("âŒ DeepSeek: Payment required")
                        return {"error": "DeepSeek billing required"}
                    elif response.status == 429:
                        print("âŒ DeepSeek: Rate limit exceeded")
                        return {"error": "DeepSeek rate limit exceeded"}
                    else:
                        error_text = await response.text()
                        print(f"âŒ DeepSeek API error {response.status}: {error_text}")
                        return {"error": f"DeepSeek API error: {response.status}"}

        except asyncio.TimeoutError:
            print("âŒ DeepSeek: Request timeout")
            return {"error": "DeepSeek API timeout"}
        except Exception as e:
            print(f"âŒ DeepSeek unexpected error: {e}")
            return {"error": f"DeepSeek error: {str(e)}"}

    async def _generate_openrouter_quiz(
        self,
        topic: str,
        difficulty: str,
        question_count: int,
        focus_areas: str
    ) -> Dict:
        """Generate quiz using OpenRouter API"""
        prompt = f"""Create {question_count} interview quiz questions about {topic}, difficulty: {difficulty}.
{f'Focus areas: {focus_areas}' if focus_areas else ''}

Return JSON array with: question, options, correctAnswer, explanation, hint, type, difficulty.
Make questions practical for job interviews."""

        try:
            print(f"ðŸ” Attempting OpenRouter quiz generation for: {topic}")

            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
                        "HTTP-Referer": settings.FRONTEND_URL,
                        "X-Title": "AjiEasy Interview Prep"
                    },
                    json={
                        "model": "mistralai/mistral-7b-instruct:free",
                        "messages": [{"role": "user", "content": prompt}],
                        "temperature": 0.7,
                        "max_tokens": 3000
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:

                    print(f"ðŸ“¡ OpenRouter Response Status: {response.status}")

                    if response.status == 200:
                        data = await response.json()
                        content = data["choices"][0]["message"]["content"]
                        print(f"ðŸ“ Raw OpenRouter Response: {content[:200]}...")

                        json_match = re.search(r'\[\s*\{.*\}\s*\]', content, re.DOTALL)
                        if json_match:
                            try:
                                questions = json.loads(json_match.group(0))
                                print(f"âœ… OpenRouter generated {len(questions)} questions")
                                return {
                                    "topic": topic,
                                    "difficulty": difficulty,
                                    "questions": questions,
                                    "totalQuestions": len(questions),
                                    "source": "openrouter"
                                }
                            except json.JSONDecodeError as e:
                                print(f"âŒ OpenRouter JSON parse error: {e}")
                                return {"error": "Failed to parse OpenRouter response"}
                        else:
                            print("âŒ No JSON array found in OpenRouter response")
                            return {"error": "No valid JSON in OpenRouter response"}

                    elif response.status == 401:
                        print("âŒ OpenRouter: Invalid API key")
                        return {"error": "OpenRouter API key invalid"}
                    elif response.status == 429:
                        print("âŒ OpenRouter: Rate limit exceeded")
                        return {"error": "OpenRouter rate limit exceeded"}
                    else:
                        error_text = await response.text()
                        print(f"âŒ OpenRouter API error {response.status}: {error_text}")
                        return {"error": f"OpenRouter API error: {response.status}"}

        except asyncio.TimeoutError:
            print("âŒ OpenRouter: Request timeout")
            return {"error": "OpenRouter API timeout"}
        except Exception as e:
            print(f"âŒ OpenRouter unexpected error: {e}")
            return {"error": f"OpenRouter error: {str(e)}"}

    async def _generate_openrouter_chat(self, messages: List[Dict]) -> str:
        """Fallback chat using OpenRouter"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {settings.OPENROUTER_API_KEY}",
                        "HTTP-Referer": settings.FRONTEND_URL,
                    },
                    json={
                        "model": "mistralai/mistral-7b-instruct:free",
                        "messages": messages,
                    },
                    timeout=aiohttp.ClientTimeout(total=30)
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data["choices"][0]["message"]["content"]
        except Exception as e:
            print(f"OpenRouter Chat Error: {e}")
        return "I'm having trouble connecting right now."

    def _generate_enhanced_local_quiz(self, topic: str, difficulty: str, question_count: int, focus_areas: str) -> Dict:
        # ... (Keep the existing local fallback logic for quiz)
        return {
            "topic": topic,
            "difficulty": difficulty,
            "questions": [
                {
                    "id": 1,
                    "question": f"Sample question about {topic}?",
                    "options": ["Option A", "Option B", "Option C", "Option D"],
                    "correctAnswer": 0,
                    "explanation": "This is a placeholder.",
                    "hint": "Try Option A",
                    "type": "technical",
                    "difficulty": difficulty
                }
            ],
            "totalQuestions": 1,
            "source": "local-fallback"
        }

# Create global instance
free_ai_service = FreeAIService()

# Wrapper functions for main.py compatibility
async def generate_questions_async(topic: str, job_description: str, interview_type: str, company_nature: str) -> List[Dict]:
    return await free_ai_service.generate_questions(topic, job_description, interview_type, company_nature)

async def generate_free_quiz(topic: str, difficulty: str = "medium", question_count: int = 10, focus_areas: str = "") -> Dict:
    return await free_ai_service.generate_quiz(topic, difficulty, question_count, focus_areas)

async def send_chat_message(topic: str, message: str, history: List[Dict] = []) -> str:
    return await free_ai_service.send_chat_message(topic, message, history)

async def generate_local_fallback_questions(topic: str, job_description: str, interview_type: str, company_nature: str) -> List[Dict]:
    # Simple fallback
    return [
        {
            "question": f"Explain the core concepts of {topic}.",
            "type": "technical",
            "difficulty": "medium",
            "explanation": "Tests fundamental knowledge."
        },
        {
            "question": f"Describe a time you used {topic} in a project.",
            "type": "behavioral",
            "difficulty": "medium",
            "explanation": "Tests experience."
        }
    ]